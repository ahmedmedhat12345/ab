{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Real Estate Machine Learning Project\n",
                "\n",
                "This notebook covers data loading, EDA, clustering, classification, regression, LLM interpretation, and Streamlit deployment.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Cleaning\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Load dataset, handle missing values, outliers, and data type conversions.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "import joblib\n",
                "import warnings\n",
                "import xgboost as xgb\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Load Dataset\n",
                "try:\n",
                "    df = pd.read_csv('data.csv')\n",
                "    print(\"Dataset loaded successfully.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Error: 'data.csv' not found. Please ensure the dataset is in the same directory.\")\n",
                "    # Create dummy data for demonstration if file missing\n",
                "    np.random.seed(42) # Reproducible\n",
                "    n_samples = 4600\n",
                "    data = {\n",
                "        'price': np.random.randint(200000, 2000000, n_samples),\n",
                "        'bedrooms': np.random.randint(1, 6, n_samples),\n",
                "        'bathrooms': np.random.randint(1, 4, n_samples),\n",
                "        'sqft_living': np.random.randint(1000, 5000, n_samples),\n",
                "        'sqft_lot': np.random.randint(2000, 10000, n_samples),\n",
                "        'floors': np.random.randint(1, 4, n_samples),\n",
                "        'waterfront': np.random.randint(0, 2, n_samples),\n",
                "        'view': np.random.randint(0, 5, n_samples),\n",
                "        'condition': np.random.randint(1, 6, n_samples),\n",
                "        'grade': np.random.randint(4, 13, n_samples),\n",
                "        'sqft_above': np.random.randint(1000, 4000, n_samples),\n",
                "        'sqft_basement': np.random.randint(0, 1000, n_samples),\n",
                "        'yr_built': np.random.randint(1950, 2023, n_samples),\n",
                "        'yr_renovated': np.random.randint(0, 2023, n_samples),\n",
                "        'zipcode': np.random.randint(98000, 98200, n_samples),\n",
                "        'lat': np.random.uniform(47.1, 47.8, n_samples),\n",
                "        'long': np.random.uniform(-122.5, -121.5, n_samples),\n",
                "        'sqft_living15': np.random.randint(1000, 4000, n_samples),\n",
                "        'sqft_lot15': np.random.randint(2000, 10000, n_samples),\n",
                "        'date': pd.date_range(start='1/1/2022', periods=n_samples)\n",
                "    }\n",
                "    # Add some correlation to make metrics non-zero\n",
                "    data['price'] = data['sqft_living'] * 300 + data['grade'] * 50000 + np.random.normal(0, 50000, n_samples)\n",
                "    \n",
                "    df = pd.DataFrame(data)\n",
                "    print(\"Created dummy dataset for demonstration with Synthetic Correlation.\")\n",
                "\n",
                "# Basic Cleaning\n",
                "df['date'] = pd.to_datetime(df['date'])\n",
                "df.dropna(inplace=True)\n",
                "\n",
                "# Remove Outliers (Simple IQR method for Price)\n",
                "Q1 = df['price'].quantile(0.25)\n",
                "Q3 = df['price'].quantile(0.75)\n",
                "IQR = Q3 - Q1\n",
                "df = df[~((df['price'] < (Q1 - 1.5 * IQR)) | (df['price'] > (Q3 + 1.5 * IQR)))]\n",
                "\n",
                "print(f\"Data shape after cleaning: {df.shape}\")\n",
                "df.head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Engineering (Enhanced)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Creating new features to improve model performance.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Engineering\n",
                "# 1. Log Transform Price (Target) to handle skewness\n",
                "df['price_log'] = np.log1p(df['price'])\n",
                "\n",
                "# 2. House Age & Renovation Status\n",
                "current_year = 2025\n",
                "df['house_age'] = current_year - df['yr_built']\n",
                "df['has_renovated'] = df['yr_renovated'].apply(lambda x: 1 if x > 0 else 0)\n",
                "\n",
                "# 3. Interactions (Grade * SqFt)\n",
                "df['grade_sqft'] = df['grade'] * df['sqft_living']\n",
                "\n",
                "# Check correlations with log price\n",
                "numeric_df = df.select_dtypes(include=[np.number])\n",
                "print(numeric_df.corr()['price_log'].sort_values(ascending=False).head(10))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Analysis & Business Insights\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Analyze price drivers and explore feature relationships.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Price Distribution (Original vs Log)\n",
                "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
                "sns.histplot(df['price'], kde=True, bins=30, ax=ax[0])\n",
                "ax[0].set_title('Original Price Distribution')\n",
                "sns.histplot(df['price_log'], kde=True, bins=30, ax=ax[1])\n",
                "ax[1].set_title('Log-Price Distribution')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Log-transformation normalizes the price distribution, helping regression models.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Unsupervised Learning (Clustering)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Group houses into meaningful categories using K-Means.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scaling\n",
                "scaler = StandardScaler()\n",
                "features_for_clustering = ['price', 'sqft_living', 'grade', 'house_age']\n",
                "X_scaled = scaler.fit_transform(df[features_for_clustering])\n",
                "\n",
                "# K-Means\n",
                "kmeans = KMeans(n_clusters=3, random_state=42)\n",
                "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
                "\n",
                "# Define Categories\n",
                "cluster_means = df.groupby('cluster')['price'].mean().sort_values()\n",
                "cluster_map = {cluster_means.index[0]: 'Budget', cluster_means.index[1]: 'Standard', cluster_means.index[2]: 'Luxury'}\n",
                "df['category'] = df['cluster'].map(cluster_map)\n",
                "\n",
                "sns.scatterplot(data=df, x='sqft_living', y='price', hue='category', palette='viridis')\n",
                "plt.title('House Clusters')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Category Classification\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Build a classifier to predict the house category for new listings.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data\n",
                "drop_cols = ['price', 'price_log', 'cluster', 'category', 'date', 'id', 'yr_renovated', 'yr_built'] \n",
                "X_cls = df.drop(drop_cols, axis=1, errors='ignore').select_dtypes(include=[np.number])\n",
                "y_cls = df['category']\n",
                "\n",
                "# Split\n",
                "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train Classifier \n",
                "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "clf.fit(X_train_c, y_train_c)\n",
                "\n",
                "# Evaluate\n",
                "acc = clf.score(X_test_c, y_test_c)\n",
                "print(f\"Classification Accuracy: {acc:.2f}\")\n",
                "\n",
                "# Save Classifier\n",
                "joblib.dump(clf, 'category_classifier.pkl')\n",
                "joblib.dump(X_cls.columns.tolist(), 'cls_features.pkl')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Price Regression Modeling (XGBoost)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Build robust regression models using XGBoost and Log-Target.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Data for Regression (Predict Log Price)\n",
                "X_reg = df.drop(drop_cols, axis=1, errors='ignore').select_dtypes(include=[np.number])\n",
                "y_reg = df['price_log'] # Target is Log Price\n",
                "\n",
                "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train XGBoost Regressor\n",
                "reg = xgb.XGBRegressor(\n",
                "    n_estimators=200, \n",
                "    learning_rate=0.05, \n",
                "    max_depth=5, \n",
                "    random_state=42, \n",
                "    n_jobs=-1\n",
                ")\n",
                "reg.fit(X_train_r, y_train_r)\n",
                "\n",
                "# Predictions (Convert back from Log scale)\n",
                "y_pred_log = reg.predict(X_test_r)\n",
                "y_pred = np.expm1(y_pred_log)\n",
                "y_test_orig = np.expm1(y_test_r)\n",
                "\n",
                "# Evaluation\n",
                "mae = mean_absolute_error(y_test_orig, y_pred)\n",
                "rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
                "r2 = r2_score(y_test_orig, y_pred)\n",
                "\n",
                "print(f\"Regression MAE: ${mae:,.0f}\")\n",
                "print(f\"Regression RMSE: ${rmse:,.0f}\")\n",
                "print(f\"R2 Score: {r2:.4f}\")\n",
                "\n",
                "# Compare with Median\n",
                "median_price = df['price'].median()\n",
                "print(f\"Market Median Price: ${median_price:,.0f}\")\n",
                "\n",
                "# Save Regressor\n",
                "joblib.dump(reg, 'price_regressor.pkl')\n",
                "joblib.dump(X_train_r.columns.tolist(), 'reg_features.pkl') # Save features used\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. LLM-based Model Interpretation\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Functions for interacting with OpenAI API with a robust manual fallback.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define functions here for notebook reference (optional, as they are mainly in Streamlit app source)\n",
                "pass\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Streamlit Deployment\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Streamlit app code with Silent Fallback Logic.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "app_code = \"\"\"\n",
                "import streamlit as st\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import numpy as np\n",
                "import xgboost as xgb\n",
                "import os\n",
                "\n",
                "# --- LLM / Manual Fallback Function ---\n",
                "def get_explanation(features, predicted_price, median_price):\n",
                "    # Manual explanation logic (Baseline)\n",
                "    diff_pct = ((predicted_price - median_price) / median_price) * 100\n",
                "    \n",
                "    manual_text = \"\"\n",
                "    if diff_pct > 10:\n",
                "        manual_text = f\"This property is valued {diff_pct:.1f}% above the market median. This premium is likely driven by superior features.\"\n",
                "    elif diff_pct < -10:\n",
                "        manual_text = f\"This property is valued {abs(diff_pct):.1f}% below the market median, potentially offering good value.\"\n",
                "    else:\n",
                "        manual_text = \"This property is valued close to the market median, reflecting standard market conditions.\"\n",
                "\n",
                "    # Try Connecting to OpenAI\n",
                "    try:\n",
                "        # Check for API ID in Secrets (Streamlit Cloud) or Env Var\n",
                "        api_key = st.secrets.get(\"OPENAI_API_KEY\") or os.getenv(\"OPENAI_API_KEY\")\n",
                "        \n",
                "        if not api_key:\n",
                "            raise ValueError(\"No API Key found\")\n",
                "\n",
                "        from openai import OpenAI\n",
                "        client = OpenAI(api_key=api_key)\n",
                "\n",
                "        prompt = f\"Explain why a house with {features} is valued at ${predicted_price:,.0f} when the median is ${median_price:,.0f}. Keep it under 50 words.\"\n",
                "        \n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": \"You are a real estate expert.\"},\n",
                "                {\"role\": \"user\", \"content\": prompt}\n",
                "            ],\n",
                "            max_tokens=60\n",
                "        )\n",
                "        return response.choices[0].message.content.strip()\n",
                "\n",
                "    except Exception as e:\n",
                "        # SILENT FALLBACK: If ANY error occurs (no key, quota, net), return manual text\n",
                "        # print(f\"LLM Error: {e}\") # Debugging only\n",
                "        return manual_text\n",
                "\n",
                "# --- Main App ---\n",
                "st.set_page_config(page_title=\"Real Estate AI\", layout=\"wide\")\n",
                "\n",
                "# Load Models\n",
                "try:\n",
                "    clf = joblib.load('category_classifier.pkl')\n",
                "    reg = joblib.load('price_regressor.pkl')\n",
                "    cls_feats = joblib.load('cls_features.pkl')\n",
                "    reg_feats = joblib.load('reg_features.pkl')\n",
                "except:\n",
                "    st.error(\"Models not found. Please run the notebook first to generate .pkl files.\")\n",
                "    st.stop()\n",
                "\n",
                "st.title(\"Real Estate Valuation AI \ud83c\udfe1\")\n",
                "st.markdown(\"Predicts house price and category, with AI-driven market insights.\")\n",
                "\n",
                "# Sidebar Inputs\n",
                "st.sidebar.header(\"Property Details\")\n",
                "input_data = {}\n",
                "\n",
                "# Raw inputs\n",
                "sqft_living = st.sidebar.number_input(\"SqFt Living\", 500, 10000, 2000)\n",
                "grade = st.sidebar.slider(\"Grade (1-13)\", 1, 13, 7)\n",
                "yr_built = st.sidebar.number_input(\"Year Built\", 1900, 2025, 2000)\n",
                "bedrooms = st.sidebar.slider(\"Bedrooms\", 1, 10, 3)\n",
                "bathrooms = st.sidebar.slider(\"Bathrooms\", 1, 5, 2)\n",
                "yr_renovated = st.sidebar.number_input(\"Year Renovated (0 if none)\", 0, 2025, 0)\n",
                "\n",
                "# Feature Engineering\n",
                "current_year = 2025\n",
                "input_dict = {\n",
                "    'sqft_living': sqft_living,\n",
                "    'grade': grade,\n",
                "    'yr_built': yr_built,\n",
                "    'bedrooms': bedrooms,\n",
                "    'bathrooms': bathrooms,\n",
                "    'yr_renovated': yr_renovated,\n",
                "    # Engineered\n",
                "    'house_age': current_year - yr_built,\n",
                "    'has_renovated': 1 if yr_renovated > 0 else 0,\n",
                "    'grade_sqft': grade * sqft_living\n",
                "}\n",
                "\n",
                "# Fill others with 0 (zipcode, etc. not in UI)\n",
                "for feat in reg_feats:\n",
                "    if feat not in input_dict:\n",
                "        input_dict[feat] = 0\n",
                "\n",
                "input_df = pd.DataFrame([input_dict])\n",
                "\n",
                "if st.sidebar.button(\"Values & Insights\"):\n",
                "    col1, col2 = st.columns(2)\n",
                "    \n",
                "    # Classification\n",
                "    try:\n",
                "        cat = clf.predict(input_df[cls_feats])[0]\n",
                "        col1.subheader(f\"Category: {cat}\")\n",
                "    except:\n",
                "        col1.warn(\"Classification unavailable\")\n",
                "    \n",
                "    # Regression\n",
                "    try:\n",
                "        price_log = reg.predict(input_df[reg_feats])[0]\n",
                "        price = np.expm1(price_log)\n",
                "        col2.metric(\"Estimated Price\", f\"${price:,.0f}\")\n",
                "        \n",
                "        # Insight\n",
                "        st.divider()\n",
                "        st.subheader(\"Market Insight\")\n",
                "        \n",
                "        median_price = 450000 # Could be loaded dynamically\n",
                "        \n",
                "        # Pass features for LLM context\n",
                "        feat_summary = f\"{sqft_living}sqft, Grade {grade}, Built {yr_built}\"\n",
                "        \n",
                "        with st.spinner(\"Generating expert analysis...\"):\n",
                "            explanation = get_explanation(feat_summary, price, median_price)\n",
                "            st.success(explanation)\n",
                "            \n",
                "    except Exception as e:\n",
                "       st.error(f\"Prediction Error: {e}\")\n",
                "\n",
                "\"\"\"\n",
                "\n",
                "with open(\"app.py\", \"w\") as f:\n",
                "    f.write(app_code)\n",
                "\n",
                "print(\"Streamlit app code output to 'app.py'.\")\n",
                "\n",
                "# Create requirements.txt for Streamlit Cloud\n",
                "reqs = \"\"\"streamlit\n",
                "pandas\n",
                "numpy\n",
                "scikit-learn\n",
                "xgboost\n",
                "joblib\n",
                "openai\n",
                "matplotlib\n",
                "seaborn\n",
                "\"\"\"\n",
                "with open(\"requirements.txt\", \"w\") as f:\n",
                "    f.write(reqs)\n",
                "print(\"Created 'requirements.txt' for Cloud deployment.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}